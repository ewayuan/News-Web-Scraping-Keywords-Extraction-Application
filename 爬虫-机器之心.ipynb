{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: use options instead of chrome_options\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "import requests\n",
    "import time\n",
    " \n",
    "import requests\n",
    "from bs4 import BeautifulSoup  # Beautiful Soup是一个可以从HTML或XML文件中提取结构化数据的Python库\n",
    "    \n",
    "option = webdriver.ChromeOptions()\n",
    "option.add_argument(\"disable-infobars\")\n",
    "option.add_argument(\"--headless\")\n",
    " \n",
    "driver = webdriver.Chrome(chrome_options = option)\n",
    "driver.get(\"https://www.jiqizhixin.com/dailies\")\n",
    "# 将网页拉倒最下方\n",
    "driver.maximize_window()\n",
    "time.sleep(2)\n",
    " \n",
    "def execute_times(times):\n",
    "    for i in range(times + 1):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(5)\n",
    "execute_times(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//*[@id=\"js-has-modal\"]/div[2]/div/div/span').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):#这里设置点击100次“加载更多”\n",
    "    driver.find_element_by_link_text(\"加载更多\").click()\n",
    "    time.sleep(5) #如果网页没有完全加载，会出现点击错误，所以加了一个睡眠时间。\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找到loading-wrapper\n",
    "loading_wrapper_items = soup.find('div', class_=re.compile('loading-wrapper'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找到所有的\"daily-every js-daily-every\"，这其中包含当前所加载出来的网页中的所有文章及日期\n",
    "temp = loading_wrapper_items.find_all('div',class_=\"daily-every js-daily-every\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# all_articles 存储在所有日期下，当前日期的所有文章title和contents\n",
    "all_articles = {}\n",
    "each_article = {}\n",
    "\n",
    "for item in temp:\n",
    "    \n",
    "    #temp = loading_wrapper_items.find_all('div',class_=\"daily-every js-daily-every\")\n",
    "    \n",
    "    # 找到所有的文章，store in article_list \n",
    "    article_list = item.find_all('div',class_=\"daily-every__list\")\n",
    "    \n",
    "    #print(type(item.find(\"div\", class_=\"daily-every__time js-daily-time \"))\n",
    "    \n",
    "    if (item.find(\"div\", class_= (\"daily-every__time js-daily-time \")) == None): \n",
    "        break\n",
    "    else:\n",
    "        # 找到当前文章的日期 daily_every_items_date\n",
    "        daily_every_items_date = item.find(\"div\", class_= (\"daily-every__time js-daily-time \")).text\n",
    "    #print(daily_every_items_date)\n",
    "    \n",
    "    # 提取当前日期下所有文章的title 存到 all_title\n",
    "    all_title = []\n",
    "    # 提取当前日期下所有文章的content 存到 all_contents\n",
    "    all_contents = []\n",
    "    # 提取当前日期下，titile及content， 存到article_day\n",
    "    article_day = []\n",
    "    for each_item in article_list:\n",
    "        \n",
    "        # 在（\"a\", class_=\"u-text-limit--two daily-every__title js-open-modal\"）中找到当前日期下所有文章的title\n",
    "        all_article_title = each_item.find_all(\"a\", class_=\"u-text-limit--two daily-every__title js-open-modal\")\n",
    "        \n",
    "        # 在（\"div\", class_=\"u-text-limit--three daily__content\"\"）中找到当前文章的content\n",
    "        all_article_contents = each_item.find_all(\"div\", class_=\"u-text-limit--three daily__content\")\n",
    "        \n",
    "        for each_article in each_item:\n",
    "            # 在all_article_title中找到每篇文章的title，“each_article_title”\n",
    "            each_article_title = each_article.find(\"a\", class_=\"u-text-limit--two daily-every__title js-open-modal\").text\n",
    "            # 在all_article_contents中找到每篇文章的content， “each_article_contents”\n",
    "            each_article_contents = each_article.find(\"div\", class_=\"u-text-limit--three daily__content\").text\n",
    "            # each_article存储每篇文章的title及content\n",
    "            each_article = {\n",
    "               \"title\" : each_article_title,\n",
    "               \"contents\" : each_article_contents\n",
    "            }\n",
    "            # 将当前的文章的title和content存到article_day list中\n",
    "            article_day.append(each_article)\n",
    "            \n",
    "    # 将当前日期下的article_day存到all_articles [daily_every_items_date]中\n",
    "    all_articles [daily_every_items_date] = article_day\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_article = {}\n",
    "\n",
    "# 从all_articles中找出符合条件的2月20-2月27日的所有文章title和contents\n",
    "for i in range(8):\n",
    "    target_article.update({\"2月2\"+ str(i): all_articles[\"2月2\"+ str(i)]})\n",
    "#print(target_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import codecs\n",
    "\n",
    "# 输出csv文件，表头为fileHeader\n",
    "fileHeader = [\"date\", \"title\",\"contents\"]\n",
    "# 写入数据\n",
    "csvFile = codecs.open(\"JiQiZhixin.csv\",\"w\",'utf_8_sig')\n",
    "writer = csv.writer(csvFile)\n",
    "writer.writerow(fileHeader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in target_article:\n",
    "    for article in target_article[date]:\n",
    "        #print(article)\n",
    "        title = article['title']\n",
    "        contens = article['contents']\n",
    "        cur_row = [date, title, contens]\n",
    "        writer.writerow(cur_row)\n",
    "csvFile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
